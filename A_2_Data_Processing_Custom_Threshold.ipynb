{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 基础工具与公共常量\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "FLAG_Good = 0             # 正常\n",
    "FLAG_Erroneous = 1        # 异常\n",
    "FLAG_Suspect = 2          # 可疑\n",
    "FLAG_Interpolated = 3     # 插值\n",
    "\n",
    "# 阈值\n",
    "Z_THR = 9\n",
    "RB_THR = 100\n",
    "df_metadata = pd.read_csv(r\"../Metadata.csv\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# (1) 频率判定 + 规范化时间轴 + 同日/同月聚合\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def detect_and_convert(df, date_col= \"date\", monthly_threshold = 2):\n",
    "    \"\"\"\n",
    "    判断日频或月频，并做规范化处理：\n",
    "    1) 将时间标准化到日期（去时分秒）；\n",
    "    2) 先按“日”聚合（同日取均值）；\n",
    "    3) 如果任一 (year, month) 的不同“日期”数 >= monthly_threshold，则判为日频(D)，否则判为月频(M)；\n",
    "    4) 月频时将日期标准化到“月初”，并再次聚合（同月取均值）。\n",
    "\n",
    "    返回: (freq, df_out) 其中 freq ∈ {'D','M'}\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    out[date_col] = pd.to_datetime(out[date_col], errors=\"coerce\").dt.normalize()\n",
    "    out = out.dropna(subset=[date_col]).sort_values(date_col)\n",
    "\n",
    "    # 先按天聚合（同日多值求均值）\n",
    "    out = out.groupby(date_col, as_index=False).mean(numeric_only=True)\n",
    "\n",
    "    # 统计每月包含的不同日期数量\n",
    "    d = out[date_col]\n",
    "    counts = d.groupby([d.dt.year, d.dt.month]).nunique()\n",
    "\n",
    "    if (counts >= monthly_threshold).any():\n",
    "        # 日频\n",
    "        freq = \"D\"\n",
    "    else:\n",
    "        # 月频：标准化到月初，再聚合一次避免同月多条\n",
    "        freq = \"M\"\n",
    "        out[date_col] = out[date_col].dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "        out = out.groupby(date_col, as_index=False).mean(numeric_only=True)\n",
    "\n",
    "    out = out.sort_values(date_col).reset_index(drop=True)\n",
    "    return freq, out\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# (2) 在连续片段内补齐规则化时间索引（跨大空档不补）\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def ensure_regular_time_index(df, date_col, freq, segment_break_d = 60, segment_break_m = 12):\n",
    "    \"\"\"\n",
    "    将各“连续片段”内部补齐规则化的时间索引（不跨大空档）：\n",
    "    - freq='D'：按日补，若相邻日期间隔 > segment_break_d 视为断段；\n",
    "    - freq='M'：按月初补，若相邻月份间隔 > segment_break_m 视为断段。\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    out[date_col] = pd.to_datetime(out[date_col], errors=\"coerce\")\n",
    "    out = out.dropna(subset=[date_col]).set_index(date_col).sort_index()\n",
    "\n",
    "    if freq.upper().startswith(\"D\"):\n",
    "        segment_break = segment_break_d\n",
    "        gaps = out.index.to_series().diff().dt.days.fillna(0).astype(int)\n",
    "        new_segment = gaps > segment_break\n",
    "\n",
    "        def make_full_index(idx):\n",
    "            return pd.date_range(idx.min().normalize(), idx.max().normalize(), freq=\"D\", name=out.index.name)\n",
    "\n",
    "    elif freq.upper().startswith(\"M\"):\n",
    "        segment_break = segment_break_m\n",
    "        this_p = out.index.to_period(\"M\")\n",
    "        prev_p = this_p.shift(1)\n",
    "        gaps = ((this_p.year - prev_p.year) * 12 + (this_p.month - prev_p.month)).fillna(0).astype(int)\n",
    "        new_segment = gaps > segment_break\n",
    "\n",
    "        def make_full_index(idx):\n",
    "            start = pd.Period(idx.min(), freq=\"M\").start_time\n",
    "            end = pd.Period(idx.max(), freq=\"M\").start_time\n",
    "            return pd.date_range(start, end, freq=\"MS\", name=out.index.name)\n",
    "    else:\n",
    "        raise ValueError(\"freq 仅支持 'D' 或 'M'\")\n",
    "\n",
    "    seg_id = new_segment.cumsum()\n",
    "    dtypes = out.dtypes\n",
    "\n",
    "    pieces = []\n",
    "    for _, seg_df in out.groupby(seg_id, sort=True):\n",
    "        full_idx = make_full_index(seg_df.index)\n",
    "        seg_re = seg_df.reindex(full_idx, copy=False)\n",
    "        # 尝试恢复原列 dtype\n",
    "        for col, dtype in dtypes.items():\n",
    "            if col in seg_re.columns:\n",
    "                try:\n",
    "                    seg_re[col] = seg_re[col].astype(dtype)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        pieces.append(seg_re)\n",
    "\n",
    "    out2 = (\n",
    "        pd.concat(pieces, axis=0)\n",
    "        .sort_index()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": date_col})\n",
    "        .sort_values(date_col)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return out2\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# (3) 标准 Z 分数——FLAG_Erroneous\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def std_z(x):\n",
    "    \"\"\"标准 Z 分数（总体标准差，ddof=0）。全 NaN 或 std=0 时返回 0。\"\"\"\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    mean = np.nanmean(x.values)\n",
    "    std = np.nanstd(x.values, ddof=0)\n",
    "    if not np.isfinite(std) or std == 0:\n",
    "        return pd.Series(np.zeros(len(x)), index=x.index)\n",
    "    return pd.Series((x - mean) / std, index=x.index)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# (4) 稳健 Z 分数（Median/MAD）——FLAG_Erroneous\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def robust_z(x, eps=1e-12):\n",
    "    \"\"\"\n",
    "    基于 Median/MAD 的稳健 Z 分数，对极端值更不敏感。\n",
    "    \"\"\"\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    if x.notna().sum() == 0:\n",
    "        return pd.Series(np.zeros(len(x)), index=x.index)\n",
    "    med = np.nanmedian(x)\n",
    "    mad = np.nanmedian(np.abs(x - med))\n",
    "    denom = 1.4826 * mad if mad > 0 else eps  # 1.4826 使正态分布下与 std 可比\n",
    "    return (x - med) / denom\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# (5) 大跳变检测与清洗（容忍平台跃迁）——FLAG_Erroneous\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def big_jump_flag_one_clean(df, val_col, flag_col, date_col=\"date\", max_gap_days=60, baseline_k=10, confirm_k=1, confirm_support=1, multiple=1.0, rebase_on_step=True ):\n",
    "    \"\"\"\n",
    "    标记“异常尖峰（spike）”，并容忍“平台跃迁（step change）”。\n",
    "\n",
    "    规则（动态阈值）：\n",
    "    - 基线 = 最近 baseline_k 个“正常点”中位数；\n",
    "    - 阈值 = multiple * |上一个正常值|（若无上一个正常值则跳过判定）；\n",
    "    - |当前值 - 基线| ≥ 阈值 → 候选跳变；\n",
    "        - 向前看 confirm_k 个点（间隔均 ≤ max_gap_days）；\n",
    "        - 若其中 ≥ confirm_support 个与“当前值”接近（差 < 阈值），则判为 step（不置异常；如 rebase_on_step=True 则切换基线）；\n",
    "        - 否则判为 spike（置 flag=1）。\n",
    "    \"\"\"\n",
    "    out = df.copy().sort_values(date_col).reset_index(drop=True)\n",
    "    out[flag_col] = out[flag_col].fillna(0).astype(int)\n",
    "\n",
    "    dt = pd.to_datetime(out[date_col], errors=\"coerce\")\n",
    "    prev_dt: Optional[pd.Timestamp] = None\n",
    "    clean_vals: deque[float] = deque(maxlen=baseline_k)\n",
    "    prev_val: Optional[float] = None\n",
    "\n",
    "    for i in range(len(out)):\n",
    "        val = out.at[i, val_col]\n",
    "        cur_dt = dt.iat[i]\n",
    "\n",
    "        # 连续性检查\n",
    "        if prev_dt is None:\n",
    "            gap_ok = False\n",
    "        else:\n",
    "            gap_ok = (cur_dt - prev_dt) <= pd.Timedelta(days=max_gap_days)\n",
    "        prev_dt = cur_dt\n",
    "\n",
    "        # 满足条件才进行判定\n",
    "        if gap_ok and len(clean_vals) > 0 and pd.notna(val):\n",
    "            baseline = float(np.median(clean_vals))\n",
    "\n",
    "            if prev_val is not None and np.isfinite(prev_val):\n",
    "                threshold_i = max(abs(prev_val) * multiple, 1e-9)  # 避免 0 阈值\n",
    "            else:\n",
    "                threshold_i = 0.0\n",
    "\n",
    "            if abs(float(val) - baseline) >= threshold_i and threshold_i > 0:\n",
    "                # 候选跳变 → 前视确认\n",
    "                support = 0\n",
    "                last_dt = cur_dt\n",
    "                for j in range(1, confirm_k + 1):\n",
    "                    if i + j >= len(out):\n",
    "                        break\n",
    "                    if (dt.iat[i + j] - last_dt) > pd.Timedelta(days=max_gap_days):\n",
    "                        break\n",
    "                    nxt = out.at[i + j, val_col]\n",
    "                    if pd.notna(nxt) and abs(float(nxt) - float(val)) < threshold_i:\n",
    "                        support += 1\n",
    "                    last_dt = dt.iat[i + j]\n",
    "\n",
    "                if support >= confirm_support:\n",
    "                    # step：不标异常，必要时重置基线\n",
    "                    if rebase_on_step:\n",
    "                        clean_vals.clear()\n",
    "                    clean_vals.append(float(val))\n",
    "                    prev_val = float(val)\n",
    "                    continue\n",
    "                else:\n",
    "                    # spike：置异常\n",
    "                    out.at[i, flag_col] = FLAG_Erroneous\n",
    "                    continue\n",
    "\n",
    "        # 正常点纳入基线\n",
    "        if out.at[i, flag_col] == FLAG_Good and pd.notna(val):\n",
    "            clean_vals.append(float(val))\n",
    "            prev_val = float(val)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# (6) Level & Storage 单调性不匹配（简单规则版）——FLAG_Suspect\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def flag_storage_to_level(df, level_col=\"level\", storage_col=\"storage\", flag_level_col=\"flag_level\", flag_storage_col=\"flag_storage\"):\n",
    "\n",
    "    out = df.copy()\n",
    "    for col in [flag_level_col, flag_storage_col]:\n",
    "        out[col] = out[col].fillna(0).astype(int)\n",
    "\n",
    "    # 只挑选两个 flag 都为 0 的数据\n",
    "    mask_ok = (out[flag_level_col].eq(0)) & (out[flag_storage_col].eq(0))\n",
    "    valid = out.loc[mask_ok, [level_col, storage_col]].dropna()\n",
    "\n",
    "    if valid.empty:\n",
    "        return out\n",
    "\n",
    "    # 按 storage 升序排序\n",
    "    sorted_vals = valid.sort_values(storage_col).reset_index()\n",
    "\n",
    "    # 检查 level 是否随 storage 单调递增\n",
    "    level_diff = sorted_vals[level_col].diff()\n",
    "\n",
    "    for i in sorted_vals.index:\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if level_diff[i] < 0:  \n",
    "            idx_bad = sorted_vals.loc[i, \"index\"]\n",
    "            # 决策：到底是 level 错，还是 storage 错？\n",
    "            # 简单规则：如果 storage 跟前后点很接近，但 level 明显偏离 → level 错\n",
    "            # 否则认为是 storage 错\n",
    "            prev_level = sorted_vals.loc[i-1, level_col]\n",
    "            cur_level = sorted_vals.loc[i, level_col]\n",
    "            prev_storage = sorted_vals.loc[i-1, storage_col]\n",
    "            cur_storage = sorted_vals.loc[i, storage_col]\n",
    "\n",
    "            if abs(cur_storage - prev_storage) < 1.0 and abs(cur_level - prev_level) > 5.0:\n",
    "                out.loc[idx_bad, flag_level_col] = FLAG_Suspect\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def flag_level_to_storage(df, level_col=\"level\", storage_col=\"storage\", flag_level_col=\"flag_level\", flag_storage_col=\"flag_storage\", abs_tol=0.1, rel_tol=0.002, side_k=5, require_both_sides=True):\n",
    "    \"\"\"\n",
    "    在 level 升序视角下，使用左右各 side_k 个邻居的中位数作为“正常参考”，\n",
    "    当当前 storage 同时偏离两侧参考值（或只要一侧，取决于 require_both_sides）超过\n",
    "      abs_tol + rel_tol * median_side\n",
    "    时，标记为异常（flag=3）。\n",
    "    \"\"\"\n",
    "    g = df.copy()\n",
    "    mask_ok = g[level_col].notna() & g[storage_col].notna() & g[flag_storage_col].eq(FLAG_Good) & g[flag_level_col].eq(FLAG_Good)\n",
    "    if mask_ok.sum() <= 2:\n",
    "        return g\n",
    "\n",
    "    tmp = g.loc[mask_ok, [level_col, storage_col]].copy()\n",
    "    order = tmp[level_col].argsort(kind=\"mergesort\").to_numpy()\n",
    "    idx_sorted = tmp.index.to_numpy()[order]\n",
    "    L = tmp[level_col].to_numpy()[order].astype(float)\n",
    "    S = tmp[storage_col].to_numpy()[order].astype(float)\n",
    "    n = S.size\n",
    "\n",
    "    bad_sorted = np.zeros(n, dtype=bool)\n",
    "\n",
    "    for i in range(n):\n",
    "        left_vals = S[max(0, i - side_k):i]\n",
    "        right_vals = S[i + 1:min(n, i + 1 + side_k)]\n",
    "\n",
    "        if left_vals.size == 0 or right_vals.size == 0:\n",
    "            continue\n",
    "\n",
    "        left_med = np.median(left_vals[np.isfinite(left_vals)])\n",
    "        right_med = np.median(right_vals[np.isfinite(right_vals)])\n",
    "\n",
    "        dl = abs(S[i] - left_med)\n",
    "        dr = abs(S[i] - right_med)\n",
    "        thr_l = abs_tol + rel_tol * max(left_med, 0.0)\n",
    "        thr_r = abs_tol + rel_tol * max(right_med, 0.0)\n",
    "\n",
    "        cond_l = dl > thr_l\n",
    "        cond_r = dr > thr_r\n",
    "\n",
    "        bad_sorted[i] = (cond_l and cond_r) if require_both_sides else (cond_l or cond_r)\n",
    "\n",
    "    if bad_sorted.any():\n",
    "        g.loc[idx_sorted[bad_sorted], flag_storage_col] = FLAG_Suspect\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# (7) 单调映射互补：storage ↔ level——FLAG_Interpolated\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def fill_by_counterpart(df, level_col=\"level\", storage_col=\"storage\", flag_level_col=\"flag_level\",flag_storage_col=\"flag_storage\"):\n",
    "    \"\"\"\n",
    "    利用“单调映射”在 flag=0 的已知样本中建立两列的经验映射，互相补值。\n",
    "    - 仅在另一列 flag=0 且当前列缺失的情况下尝试补值；\n",
    "    - 命中精确/四舍五入匹配，否则用“邻区平均”兜底；\n",
    "    - 成功补值且原 flag=0 → 置 flag=3（FLAG_Interpolated）。\n",
    "    \"\"\"\n",
    "    g = df.copy()\n",
    "\n",
    "    clean_both = g[flag_level_col].eq(FLAG_Good) & g[flag_storage_col].eq(FLAG_Good)\n",
    "\n",
    "    # storage -> level（两列都干净）\n",
    "    base_L = g.loc[clean_both, [level_col, storage_col]].dropna()\n",
    "    s2l = base_L.groupby(storage_col)[level_col].median().sort_index()\n",
    "\n",
    "    # level -> storage（两列都干净）\n",
    "    base_S = g.loc[clean_both, [level_col, storage_col]].dropna()\n",
    "    l2s = base_S.groupby(level_col)[storage_col].median().sort_index()\n",
    "\n",
    "\n",
    "    def _round_layers_lookup(val, src, tgt, okmask):\n",
    "        for rd in (2, 1, 0):\n",
    "            m = okmask & g[src].notna() & g[tgt].notna() & (g[src].round(rd) == round(val, rd))\n",
    "            if m.any():\n",
    "                return g.loc[m, tgt].median()\n",
    "        return np.nan\n",
    "\n",
    "    def _bracket_average(series_map: pd.Series, val: float):\n",
    "        if pd.isna(val) or series_map.empty:\n",
    "            return np.nan\n",
    "        ix = series_map.index.to_numpy(dtype=float)\n",
    "        pos = np.searchsorted(ix, val, side=\"left\")\n",
    "\n",
    "        left_v = series_map.loc[ix[pos - 1]] if pos > 0 else np.nan\n",
    "        right_v = series_map.loc[ix[pos]] if pos < ix.size else np.nan\n",
    "\n",
    "        if np.isfinite(left_v) and np.isfinite(right_v):\n",
    "            return 0.5 * (left_v + right_v)\n",
    "        return left_v if np.isfinite(left_v) else (right_v if np.isfinite(right_v) else np.nan)\n",
    "\n",
    "    def _find_level_from_storage(s):\n",
    "        if s in s2l.index:\n",
    "            return s2l.loc[s]\n",
    "        v = _round_layers_lookup(s, storage_col, level_col, g[flag_storage_col].eq(FLAG_Good))\n",
    "        return v if pd.notna(v) else _bracket_average(s2l, float(s))\n",
    "\n",
    "    def _find_storage_from_level(lv):\n",
    "        if lv in l2s.index:\n",
    "            return l2s.loc[lv]\n",
    "        v = _round_layers_lookup(lv, level_col, storage_col, g[flag_level_col].eq(FLAG_Good))\n",
    "        return v if pd.notna(v) else _bracket_average(l2s, float(lv))\n",
    "\n",
    "    # 补 level（要求 storage 有值且 storage 的 flag=Good）\n",
    "    need_L = g[level_col].isna() & g[storage_col].notna()# & g[flag_storage_col].eq(FLAG_Good)\n",
    "    for i, row in g.loc[need_L].iterrows():\n",
    "        nv = _find_level_from_storage(row[storage_col])\n",
    "        if pd.notna(nv):\n",
    "            g.at[i, level_col] = nv\n",
    "            if g.at[i, flag_level_col] == FLAG_Good:\n",
    "                g.at[i, flag_level_col] = FLAG_Interpolated\n",
    "\n",
    "    # 补 storage（要求 level 有值且 level 的 flag≠Erroneous--后续对 level 和 storage 都缺失的情况下进行单列插值）\n",
    "    need_S = g[storage_col].isna() & g[level_col].notna()# & ~g[flag_level_col].eq(FLAG_Erroneous)\n",
    "    for i, row in g.loc[need_S].iterrows():\n",
    "        nv = _find_storage_from_level(row[level_col])\n",
    "        if pd.notna(nv):\n",
    "            g.at[i, storage_col] = nv\n",
    "            if g.at[i, flag_storage_col] == FLAG_Good:\n",
    "                g.at[i, flag_storage_col] = FLAG_Interpolated\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# (8) 用 storage 推断缺失的 level——FLAG_Interpolated\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def fill_level_from_storage(df, storage_col=\"storage\", level_col=\"level\", flag_level_col=\"flag_level\", storage_tol=0.0 ):\n",
    "    \"\"\"\n",
    "    仅在当前行 level 缺失且 storage 有值时尝试补值：\n",
    "    1) 先看“相同 storage（容差内）”的已知 level：\n",
    "       - 命中 1 个 → 直接用；\n",
    "       - 命中 2 个 → 取均值；\n",
    "       - 命中 ≥3 个 → 取中位数；\n",
    "    2) 否则在“已知 (storage, level)”样本的 storage 升序上，找左右邻居：\n",
    "       - 若左右均存在且 storage 不同 → 线性插值，并将 flag_level=2；\n",
    "       - 若仅一侧存在 → 取该侧 level；\n",
    "       - 否则不填。\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    out[flag_level_col] = out[flag_level_col].fillna(FLAG_Good).astype(int)\n",
    "\n",
    "    # 仅用“已知 level & storage”的点作为参考\n",
    "    known_mask = out[level_col].notna() & out[storage_col].notna()\n",
    "    if known_mask.sum() == 0:\n",
    "        return out\n",
    "\n",
    "    known = out.loc[known_mask, [storage_col, level_col]]\n",
    "    order = np.argsort(known[storage_col].to_numpy(dtype=float))\n",
    "    ks = known[storage_col].to_numpy(dtype=float)[order]\n",
    "    kl = known[level_col].to_numpy(dtype=float)[order]\n",
    "\n",
    "    miss_mask = out[level_col].isna() & out[storage_col].notna()\n",
    "\n",
    "    for idx in out.index[miss_mask]:\n",
    "        s = float(out.at[idx, storage_col])\n",
    "\n",
    "        # 1) 相同 storage\n",
    "        exact_hits = np.isclose(ks, s, atol=storage_tol, rtol=0.0) if storage_tol > 0 else (ks == s)\n",
    "        if np.any(exact_hits):\n",
    "            vals = kl[exact_hits]\n",
    "            n = vals.size\n",
    "            fill_val = float(vals[0]) if n == 1 else (float((vals[0] + vals[1]) / 2.0) if n == 2 else float(np.median(vals)))\n",
    "            out.at[idx, level_col] = fill_val\n",
    "            continue\n",
    "\n",
    "        # 2) 上下邻居\n",
    "        pos = np.searchsorted(ks, s, side=\"left\")\n",
    "        left_ok = (pos - 1) >= 0\n",
    "        right_ok = pos < ks.size\n",
    "\n",
    "        if left_ok and right_ok:\n",
    "            sL, lL = ks[pos - 1], kl[pos - 1]\n",
    "            sR, lR = ks[pos], kl[pos]\n",
    "\n",
    "            if not np.isclose(sL, sR):\n",
    "                t = (s - sL) / (sR - sL)\n",
    "                out.at[idx, level_col] = float(lL + t * (lR - lL))\n",
    "                out.at[idx, flag_level_col] = FLAG_Interpolated\n",
    "            else:\n",
    "                out.at[idx, level_col] = float(lL if abs(s - sL) <= abs(s - sR) else lR)\n",
    "        elif left_ok:\n",
    "            out.at[idx, level_col] = float(kl[pos - 1])\n",
    "        elif right_ok:\n",
    "            out.at[idx, level_col] = float(kl[pos])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# (9) 统一时间插值（只补内部缺口；可选补边缘）——FLAG_Interpolated\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def final_time_interp(df, date_col=\"date\", val_col=\"level\", flag_col=\"flag_level\", limit=3, fill_edges=False):\n",
    "    \"\"\"\n",
    "    在规则化时间轴上，对指定列做线性插值：\n",
    "    - 默认仅补内部缺口（不外推首尾）；\n",
    "    - 成功补值且原 flag=0 → 置 flag=2。\n",
    "    \"\"\"\n",
    "    g = df.copy()\n",
    "    g[date_col] = pd.to_datetime(g[date_col], errors=\"coerce\")\n",
    "    g = g.dropna(subset=[date_col]).sort_values(date_col).set_index(date_col)\n",
    "\n",
    "    if isinstance(g.index, pd.PeriodIndex):\n",
    "        g.index = g.index.to_timestamp(how=\"start\")\n",
    "    else:\n",
    "        g.index = pd.to_datetime(g.index, errors=\"coerce\")\n",
    "        g = g[~g.index.isna()]\n",
    "        if getattr(g.index, \"tz\", None) is not None:\n",
    "            g.index = g.index.tz_localize(None)\n",
    "\n",
    "    if val_col in g.columns:\n",
    "        before = g[val_col].isna()\n",
    "\n",
    "        # ① 仅内部插值（limit_area=\"inside\"）\n",
    "        g[val_col] = g[val_col].interpolate(method=\"linear\", limit=limit, limit_area=\"inside\")\n",
    "        filled_inside = before & g[val_col].notna()\n",
    "        m = filled_inside & ~g[flag_col].isin([FLAG_Erroneous, FLAG_Suspect])\n",
    "        g.loc[m, flag_col] = FLAG_Interpolated\n",
    "\n",
    "        # ② 可选补边缘（前/后向内），仍不做外推\n",
    "        if fill_edges:\n",
    "            before_edges = g[val_col].isna()\n",
    "            g[val_col] = g[val_col].interpolate(method=\"linear\", limit=limit, limit_direction=\"forward\")\n",
    "            g[val_col] = g[val_col].interpolate(method=\"linear\", limit=limit, limit_direction=\"backward\")\n",
    "            filled_edge = before_edges & g[val_col].notna()\n",
    "            m = filled_edge & ~g[flag_col].isin([FLAG_Erroneous, FLAG_Suspect])\n",
    "            g.loc[m, flag_col] = FLAG_Interpolated\n",
    "\n",
    "    return g.reset_index().rename(columns={\"index\": date_col})\n",
    "\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# 单变量质控函数\n",
    "# ===========================================\n",
    "\n",
    "def qc_level_or_storage(df, station_col, date_col, value_col):\n",
    "    \"\"\"\n",
    "    针对单一变量（level 或 storage）的质控与插补。\n",
    "    \"\"\"\n",
    "\n",
    "    name = value_col.lower()\n",
    "    if \"level\" in name:\n",
    "        var, flag, raw = \"level\", \"flag_level\", \"level_raw\"\n",
    "    elif \"stor\" in name:\n",
    "        var, flag, raw = \"storage\", \"flag_storage\", \"storage_raw\"\n",
    "    else:\n",
    "        var, flag, raw = \"level\", \"flag_level\", \"level_raw\"\n",
    "\n",
    "    rename_map = {station_col: \"id\", date_col: \"date\", value_col: var}\n",
    "    g = df.rename(columns=rename_map).copy()[[\"id\", \"date\", var]]\n",
    "\n",
    "    id_val = g['id'].unique().item()\n",
    "    g[var] = pd.to_numeric(g[var], errors=\"coerce\")\n",
    "    g[raw] = g[var]\n",
    "    g[flag] = FLAG_Good\n",
    "    g.loc[g[var] == 0, var] = np.nan\n",
    "    \n",
    "    frequency, g = detect_and_convert(g, \"date\")\n",
    "    max_gap = 60 if str(frequency).upper().startswith('D') else 12\n",
    "\n",
    "    # 标准差检测\n",
    "    g[f\"std_{var}\"] = std_z(g[var]).abs()\n",
    "    g.loc[g[f\"std_{var}\"] > Z_THR, flag] = FLAG_Erroneous\n",
    "    g.loc[g[flag].isin([FLAG_Erroneous, FLAG_Suspect]), \"level\"] = np.nan\n",
    "    \n",
    "    # 稳健标准差检测\n",
    "    g[f\"rb_{var}\"] = np.nan\n",
    "    g[f\"rb_{var}\"] = robust_z(g[var]).abs()\n",
    "    g.loc[g[f\"rb_{var}\"] > RB_THR, flag] = FLAG_Erroneous\n",
    "    g.loc[g[flag].isin([FLAG_Erroneous, FLAG_Suspect]), var] = np.nan\n",
    "    \n",
    "    # 跳变检测\n",
    "    g = big_jump_flag_one_clean(g, val_col=var, flag_col=flag, max_gap_days=max_gap, baseline_k=5)\n",
    "    g.loc[g[flag].isin([FLAG_Erroneous, FLAG_Suspect]), var] = np.nan\n",
    "    \n",
    "    # 时间规则化与插值\n",
    "    g = ensure_regular_time_index(g, \"date\", frequency)\n",
    "    g[flag] = g[flag].fillna(FLAG_Interpolated)\n",
    "    g = final_time_interp(g, val_col=var, flag_col=flag, limit=max_gap)\n",
    "\n",
    "    # 标记已插补\n",
    "    g.loc[g[raw].isna() & g[var].notna(), flag] = FLAG_Interpolated\n",
    "    g = g.dropna(subset=[var], how=\"all\")\n",
    "    \n",
    "    g[\"id\"] = id_val\n",
    "    cols_out = [\"id\", \"date\", raw, flag, var]\n",
    "    return g[cols_out]\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# 双变量质控函数\n",
    "# ===========================================\n",
    "\n",
    "def qc_level_and_storage(df, station_col, date_col, level_col, storage_col):\n",
    "    \"\"\"\n",
    "    对同一测站的水位(Level)和库容(Storage)时间序列进行联合质控与插补。\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- 1) 重命名列 ----\n",
    "    rename_cols = {station_col: \"id\", date_col: \"date\", level_col: \"level\", storage_col: \"storage\"}\n",
    "    g = df.rename(columns=rename_cols).copy()[[\"id\", \"date\", \"level\", \"storage\"]]\n",
    "    id_val = g['id'].unique().item()\n",
    "\n",
    "    # ---- 2) 转换类型 & 初始化 ----\n",
    "    g['level'] = pd.to_numeric(g['level'], errors='coerce')\n",
    "    g['storage'] = pd.to_numeric(g['storage'], errors='coerce')\n",
    "    g[\"level_raw\"] = g[\"level\"]\n",
    "    g[\"storage_raw\"] = g[\"storage\"]\n",
    "    g[\"flag_level\"] = FLAG_Good\n",
    "    g[\"flag_storage\"] = FLAG_Good\n",
    "\n",
    "    # 零值设为缺测\n",
    "    g.loc[g[\"level\"] == 0, \"level\"] = np.nan\n",
    "    g.loc[g[\"storage\"] == 0, \"storage\"] = np.nan\n",
    "\n",
    "    g.loc[g[\"level_raw\"] == 0, \"flag_level\"] = FLAG_Erroneous\n",
    "    g.loc[g[\"level_raw\"] == 0, \"flag_storage\"] = FLAG_Erroneous\n",
    "    \n",
    "    # ---- 3) 检测频率 ----\n",
    "    frequency, g = detect_and_convert(g, \"date\")\n",
    "    max_gap = 60 if str(frequency).upper().startswith('D') else 12\n",
    "\n",
    "    # ---- 4) 标准差检测（std_z）----FLAG_Erroneous\n",
    "    g[\"std_level\"] = std_z(g[\"level\"]).abs()\n",
    "    g[\"std_storage\"] = std_z(g[\"storage\"]).abs()\n",
    "    g.loc[g[\"std_level\"] > Z_THR, \"flag_level\"] = FLAG_Erroneous\n",
    "    g.loc[g[\"std_storage\"] > Z_THR, \"flag_storage\"] = FLAG_Erroneous\n",
    "    \n",
    "    g.loc[g[\"flag_level\"].isin([FLAG_Erroneous, FLAG_Suspect]), \"level\"] = np.nan\n",
    "    g.loc[g[\"flag_storage\"].isin([FLAG_Erroneous, FLAG_Suspect]), \"storage\"] = np.nan\n",
    "\n",
    "    # ---- 5) 稳健标准差检测（robust_z）----FLAG_Erroneous\n",
    "    g[\"rb_level\"] = np.nan\n",
    "    g[\"rb_storage\"] = np.nan\n",
    "    \n",
    "    g[\"rb_level\"] = robust_z(g[\"level\"]).abs()\n",
    "    g[\"rb_storage\"] = robust_z(g[\"storage\"]).abs()\n",
    "    g.loc[g[\"rb_level\"] > RB_THR, \"flag_level\"] = FLAG_Erroneous\n",
    "    g.loc[g[\"rb_storage\"] > RB_THR, \"flag_storage\"] = FLAG_Erroneous\n",
    "    \n",
    "    g.loc[g[\"flag_level\"].isin([FLAG_Erroneous, FLAG_Suspect]), \"level\"] = np.nan\n",
    "    g.loc[g[\"flag_storage\"].isin([FLAG_Erroneous, FLAG_Suspect]), \"storage\"] = np.nan\n",
    "    \n",
    "    # ---- 6) 跳变检测 ----FLAG_Erroneous\n",
    "    source = df_metadata.loc[df_metadata[\"RES_ID\"] == int(id_val), \"Source\"].iloc[0]\n",
    "    if source == 'DAHITI':\n",
    "        g = big_jump_flag_one_clean(g, val_col=\"level\", flag_col=\"flag_level\", max_gap_days=max_gap, baseline_k=5)\n",
    "    else:\n",
    "        g = big_jump_flag_one_clean(g, val_col=\"level\", flag_col=\"flag_level\", max_gap_days=max_gap, baseline_k=5)\n",
    "        g = big_jump_flag_one_clean(g, val_col=\"storage\", flag_col=\"flag_storage\", max_gap_days=max_gap, baseline_k=5, multiple=3)\n",
    "\n",
    "    g.loc[g[\"flag_level\"].isin([FLAG_Erroneous, FLAG_Suspect]), \"level\"] = np.nan\n",
    "    g.loc[g[\"flag_storage\"].isin([FLAG_Erroneous, FLAG_Suspect]), \"storage\"] = np.nan\n",
    "\n",
    "    # ---- 7) Level 与 Storage 对应关系异常 ----FLAG_Suspect\n",
    "    g = flag_storage_to_level(g, level_col=\"level\", storage_col=\"storage\", flag_level_col=\"flag_level\", flag_storage_col=\"flag_storage\")\n",
    "    # g = flag_level_to_storage(g,level_col=\"level\", storage_col=\"storage\", flag_level_col=\"flag_level\", flag_storage_col=\"flag_storage\")\n",
    "\n",
    "    g.loc[g[\"flag_level\"].isin([FLAG_Erroneous, FLAG_Suspect]), \"level\"] = np.nan\n",
    "    g.loc[g[\"flag_storage\"].isin([FLAG_Erroneous, FLAG_Suspect]), \"storage\"] = np.nan\n",
    "\n",
    "    # ---- 8) 规则化时间索引 ----\n",
    "    g = ensure_regular_time_index(g, \"date\", frequency)\n",
    "    g[\"flag_level\"]   = g[\"flag_level\"].fillna(FLAG_Good)\n",
    "    g[\"flag_storage\"] = g[\"flag_storage\"].fillna(FLAG_Good)\n",
    "\n",
    "    # ---- 9) 互补：仅在两 flag=0 的行，用单调映射互补 ----\n",
    "    g = fill_by_counterpart(g)\n",
    "\n",
    "    # ---- 10) 对level缺失，但storage存在的进行插值，因为 9) 不能全部补全，这个时候不能随便插，要考虑storage ----\n",
    "    g = fill_level_from_storage(g, storage_col=\"storage\", level_col=\"level\", flag_level_col=\"flag_level\", storage_tol=1e-6)\n",
    "\n",
    "    # ---- 11) 对 level 和 storage 都缺失或者利用对应关系也不能补全的进行插值补全 ----\n",
    "    g = final_time_interp(g, val_col=\"level\", flag_col=\"flag_level\", limit=max_gap)\n",
    "    g = fill_by_counterpart(g)\n",
    "\n",
    "    # ---- 12) 标记已插补 ----\n",
    "    mL = g[\"level_raw\"].isna()   & g[\"level\"].notna()    & ~g[\"flag_level\"].isin([FLAG_Erroneous, FLAG_Suspect])\n",
    "    g.loc[mL, \"flag_level\"] = FLAG_Interpolated\n",
    "\n",
    "    mS = g[\"storage_raw\"].isna() & g[\"storage\"].notna()  & ~g[\"flag_storage\"].isin([FLAG_Erroneous, FLAG_Suspect])\n",
    "    g.loc[mS, \"flag_storage\"] = FLAG_Interpolated\n",
    "\n",
    "    # ---- 13) 清理输出 ----\n",
    "    g = g.dropna(subset=[\"level\", \"storage\"], how=\"all\")\n",
    "    g[\"id\"] = id_val\n",
    "    cols_out = [\"id\", \"date\", \"level_raw\", \"flag_level\", \"level\", \"storage_raw\", \"flag_storage\", \"storage\"]\n",
    "\n",
    "    return g[cols_out]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-20T03:18:40.607868100Z",
     "start_time": "2025-10-20T03:18:40.364341300Z"
    }
   },
   "id": "ac68da0e7cab0865",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "        ID       Date  level_raw  Flag_Level    Level\n0     4245 1992-11-30     356.94         0.0  356.940\n1     4245 1992-12-01        NaN         3.0  356.930\n2     4245 1992-12-02        NaN         3.0  356.920\n3     4245 1992-12-03        NaN         3.0  356.910\n4     4245 1992-12-04        NaN         3.0  356.900\n...    ...        ...        ...         ...      ...\n6844  4245 2022-04-02        NaN         3.0  356.758\n6845  4245 2022-04-03        NaN         3.0  356.781\n6846  4245 2022-04-04        NaN         3.0  356.804\n6847  4245 2022-04-05        NaN         3.0  356.827\n6848  4245 2022-04-06     356.85         0.0  356.850\n\n[6849 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Date</th>\n      <th>level_raw</th>\n      <th>Flag_Level</th>\n      <th>Level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4245</td>\n      <td>1992-11-30</td>\n      <td>356.94</td>\n      <td>0.0</td>\n      <td>356.940</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4245</td>\n      <td>1992-12-01</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>356.930</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4245</td>\n      <td>1992-12-02</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>356.920</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4245</td>\n      <td>1992-12-03</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>356.910</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4245</td>\n      <td>1992-12-04</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>356.900</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6844</th>\n      <td>4245</td>\n      <td>2022-04-02</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>356.758</td>\n    </tr>\n    <tr>\n      <th>6845</th>\n      <td>4245</td>\n      <td>2022-04-03</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>356.781</td>\n    </tr>\n    <tr>\n      <th>6846</th>\n      <td>4245</td>\n      <td>2022-04-04</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>356.804</td>\n    </tr>\n    <tr>\n      <th>6847</th>\n      <td>4245</td>\n      <td>2022-04-05</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>356.827</td>\n    </tr>\n    <tr>\n      <th>6848</th>\n      <td>4245</td>\n      <td>2022-04-06</td>\n      <td>356.85</td>\n      <td>0.0</td>\n      <td>356.850</td>\n    </tr>\n  </tbody>\n</table>\n<p>6849 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#29 3549\n",
    "df = pd.read_csv(r\"D:\\Zhang-MY\\Python-Code\\paper\\全球水库水位数据下载和处理\\Global Reservoir Long Time Series Water Level and Water Storage Data\\4245.csv\",dtype={0: str})\n",
    "dfff = qc_level_or_storage(df, 'RES_ID', 'Date', 'Level').rename(columns={'id':'ID','date':'Date','level':'Level','flag_level':'Flag_Level','storage':'Storage','flag_storage':'Flag_Storage'})\n",
    "dfff\n",
    "\n",
    "# df = pd.read_csv(r\"D:\\Zhang-MY\\Python-Code\\paper\\全球水库水位数据下载和处理\\Global Reservoir Long Time Series Water Level and Water Storage Data\\196.csv\",dtype={0: str})\n",
    "# dfff = qc_one_dataframe(df, 'RES_ID', 'Date', 'Storage').rename(columns={'id':'ID','date':'Date','level':'Level','flag_level':'Flag_Level','storage':'Storage','flag_storage':'Flag_Storage'})\n",
    "# dfff\n",
    "\n",
    "# /1022/69/1006/1003   1414没有同时存在level和storage的数据  检验最后一个插值37   3712、3427\n",
    "# df = pd.read_csv(r\"D:\\Zhang-MY\\Python-Code\\paper\\全球水库水位数据下载和处理\\Global Reservoir Long Time Series Water Level and Water Storage Data\\37.csv\")\n",
    "# dfff = qc_level_and_storage(df, 'RES_ID', 'Date', 'Level','Storage').rename(columns={'id':'RES_ID','date':'Date','level_raw':'Level_Raw','level':'Level','flag_level':'Flag_Level','storage_raw':'Storage_Raw','storage':'Storage','flag_storage':'Flag_Storage'})\n",
    "# dfff.to_csv(r\"D:\\Desktop\\aaa.csv\",index = False)\n",
    "# # dfff = dfff[dfff['Date'] > '1988-09-01']\n",
    "# dfff"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-20T03:19:24.103590900Z",
     "start_time": "2025-10-20T03:19:23.999717700Z"
    }
   },
   "id": "7095b303c18adff0",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "folder_in = r\"../Global Reservoir Long Time Series Water Level and Water Storage Data/\"\n",
    "bad_id = []\n",
    "files = [f for f in os.listdir(folder_in) if f.endswith(\".csv\")]\n",
    "\n",
    "for file in tqdm(files, desc=\"Processing CSV files\", unit=\"file\"):\n",
    "    file_id = os.path.splitext(file)[0]  # 提取文件名（RES_ID_raw）\n",
    "    path = os.path.join(folder_in, file)\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.replace(\"0\", np.nan).replace(0, np.nan)\n",
    "    \n",
    "    frequency, g = detect_and_convert(df, \"Date\")\n",
    "    max_gap = 30 if str(frequency).upper().startswith('D') else 12\n",
    "\n",
    "    if \"Level\" in df.columns and \"Storage\" in df.columns:\n",
    "        length = max(len(df[\"Level\"].dropna()), len(df[\"Storage\"].dropna()))\n",
    "    elif \"Level\" in df.columns:\n",
    "        length = len(df[\"Level\"].dropna())\n",
    "    elif \"Storage\" in df.columns:\n",
    "        length = len(df[\"Storage\"].dropna())\n",
    "    else:\n",
    "        length = 0\n",
    "        \n",
    "    if length < max_gap:\n",
    "        bad_id.append(int(file_id))\n",
    "        continue\n",
    "print(f\"数据过少 {len(bad_id)} 个：\")\n",
    "\n",
    "df_Metadata = pd.read_csv(r\"../Metadata.csv\")\n",
    "df_Metadata = df_Metadata[~df_Metadata[\"RES_ID\"].isin(bad_id)].rename(columns={\"RES_ID\":\"RES_ID_raw\"})\n",
    "df_Metadata[\"RES_ID\"] = range(1, len(df_Metadata) + 1)\n",
    "df_Metadata.to_csv(\"../Metadata.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_Metadata"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-20T03:18:41.717788800Z",
     "start_time": "2025-10-20T03:18:41.707675100Z"
    }
   },
   "id": "1da27b36ddfeb082",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "df_Metadata = pd.read_csv(r\"../Metadata.csv\")\n",
    "folder_in = r\"../Global Reservoir Long Time Series Water Level and Water Storage Data/\"\n",
    "folder_out = r\"../Global Reservoir Long Time Series Water Level and Water Storage Data New/\"\n",
    "os.makedirs(folder_out, exist_ok=True)\n",
    "\n",
    "for _, row in df_Metadata.iterrows():\n",
    "    old_name = f\"{row['RES_ID_raw']}.csv\"\n",
    "    new_name = f\"{row['RES_ID']}.csv\"\n",
    "    \n",
    "    old_path = os.path.join(folder_in, old_name)\n",
    "    new_path = os.path.join(folder_out, new_name)\n",
    "    \n",
    "    if os.path.exists(old_path):\n",
    "        shutil.copy2(old_path, new_path)  # ✅ 复制文件\n",
    "        \n",
    "        # ✅ 修改 CSV 内部的 RES_ID 列为新文件名（去掉 .csv）\n",
    "        try:\n",
    "            df = pd.read_csv(new_path)\n",
    "            if \"RES_ID\" in df.columns:\n",
    "                df[\"RES_ID\"] = row[\"RES_ID\"]  # 或者用 new_name.replace(\".csv\", \"\")\n",
    "                df.to_csv(new_path, index=False)\n",
    "            else:\n",
    "                print(f\"⚠️ 文件 {new_name} 中未找到 'RES_ID' 列，跳过修改。\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 修改 {new_name} 时出错：{e}\")\n",
    "    else:\n",
    "        print(f\"⚠️ 未找到文件：{old_name}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-20T02:51:04.503881400Z",
     "start_time": "2025-10-20T02:49:01.027210800Z"
    }
   },
   "id": "7ec89e1369eeea07",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files: 100%|██████████| 4200/4200 [1:42:52<00:00,  1.47s/file]  \n"
     ]
    },
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "folder_in = r\"../Global Reservoir Long Time Series Water Level and Water Storage Data New/\"\n",
    "folder_out = r\"../GROWL/GROWL_timeseries/\"\n",
    "os.makedirs(folder_out, exist_ok=True)\n",
    "bad_id = []\n",
    "files = [f for f in os.listdir(folder_in) if f.endswith(\".csv\")]\n",
    "existing_ids = {os.path.splitext(f)[0] for f in os.listdir(folder_out) if f.endswith(\".csv\")}\n",
    "\n",
    "for file in tqdm(files, desc=\"Processing CSV files\", unit=\"file\"):\n",
    "    file_id = os.path.splitext(file)[0]\n",
    "    if file_id in existing_ids:\n",
    "        continue\n",
    "        \n",
    "    path = os.path.join(folder_in, file)\n",
    "    # print(f\"正在处理 {os.path.basename(file)}\")\n",
    "    df = pd.read_csv(path).replace(0, np.nan)\n",
    "\n",
    "    has_level = \"Level\" in df.columns and df[\"Level\"].nunique() > 1\n",
    "    has_storage = \"Storage\" in df.columns and df[\"Storage\"].nunique() > 1\n",
    "\n",
    "    # 按列的存在情况调用 qc_and_dataframe / qc_one_dataframe\n",
    "    if has_level and has_storage:\n",
    "        df_clear = qc_level_and_storage(df, 'RES_ID', \"Date\", \"Level\", \"Storage\")\n",
    "    elif has_level:\n",
    "        df_clear = qc_level_or_storage(df, 'RES_ID', \"Date\", \"Level\")\n",
    "    elif has_storage:\n",
    "        df_clear = qc_level_or_storage(df, 'RES_ID', \"Date\", \"Storage\")\n",
    "    \n",
    "    else:\n",
    "        bad_id.append(os.path.splitext(file)[0])\n",
    "        continue\n",
    "\n",
    "    df_clear = df_clear.rename(columns={\n",
    "        \"id\": 'RES_ID',\n",
    "        \"date\": \"Date\",\n",
    "        \"level_raw\": \"Level_Raw\",\n",
    "        \"flag_level\": \"Flag_Level\",\n",
    "        \"level\": \"Level\",\n",
    "        \"storage_raw\": \"Storage_Raw\",\n",
    "        \"storage\": \"Storage\",\n",
    "        \"flag_storage\": \"Flag_Storage\",\n",
    "    })\n",
    "    df_clear['RES_ID'] = df_clear['RES_ID'].astype(str)\n",
    "    \n",
    "    if \"Storage\" in df_clear.columns and df_clear[\"Storage\"].isna().all():\n",
    "        df_clear = df_clear.drop(columns=[\"Storage\",\"Flag_Storage\"])\n",
    "    if \"Level\" in df_clear.columns and df_clear[\"Level\"].isna().all():\n",
    "        df_clear = df_clear.drop(columns=[\"Level\",\"Flag_Level\"])\n",
    "        \n",
    "    out_path = os.path.join(folder_out, file)\n",
    "    df_clear.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "bad_id"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-18T10:55:15.066900100Z",
     "start_time": "2025-10-18T09:12:22.697973500Z"
    }
   },
   "id": "8b05b58df955f0b9",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# \n",
    "# folder_path = r\"../GROWL/GROWL_timeseries/\"\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.endswith(\".csv\"):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         # 读取 csv\n",
    "#         g = pd.read_csv(file_path).rename(columns={\n",
    "#             \"id\": 'RES_ID',\n",
    "#             \"date\": \"Date\",\n",
    "#             \"level_raw\": \"Level_Raw\",\n",
    "#             \"flag_level\": \"Flag_Level\",\n",
    "#             \"level\": \"Level\",\n",
    "#             \"storage_raw\": \"Storage_Raw\",\n",
    "#             \"storage\": \"Storage\",\n",
    "#             \"flag_storage\": \"Flag_Storage\"})\n",
    "#         \n",
    "#         g['RES_ID'] = int(os.path.splitext(filename)[0])\n",
    "# \n",
    "#         cols_out = ['RES_ID', \"Date\",\"Level_Raw\", \"Flag_Level\", \"Level\",\"Storage_Raw\", \"Flag_Storage\", \"Storage\"]\n",
    "#         cols_present = [c for c in cols_out if c in g.columns]\n",
    "#         g = g[cols_present]\n",
    "#         g.to_csv(file_path, index=False)\n",
    "# \n",
    "#         print(f\"✅ 已处理: {filename}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "763209741b672472",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      RES_ID   Station                                       Name   Latitude  \\\n0          1    001886                                 Kagalurpak  60.970000   \n1          2    000497                                       Dall  60.330000   \n2          3    001469                                Baird_Inlet  60.790000   \n3          4    001063                                 Nunavakpak  60.770000   \n4          5  16094150  Ka Loko Reservoir near Kilauea, Kauai, HI  22.178968   \n...      ...       ...                                        ...        ...   \n4195    4196   142108A      Sideling Ck at Lake Kurwongbah Dam HW -27.250000   \n4196    4197   146033A                   Nerang R at Hinze Dam HW -28.050000   \n4197    4198   146034A                              Little Nerang -28.145197   \n4198    4199    100265                                   Wakatipu -44.960000   \n4199    4200    000511                                      Taupo -38.800000   \n\n       Longitude Temporal_Resolution  \\\n0    -164.010000               Daily   \n1    -163.800000               Daily   \n2    -163.490000               Daily   \n3    -162.570000               Daily   \n4    -159.379061               Daily   \n...          ...                 ...   \n4195  152.950000               Daily   \n4196  153.280000               Daily   \n4197  153.284782               Daily   \n4198  168.400000               Daily   \n4199  176.030000               Daily   \n\n                                          Level_Periods  \\\n0     1992-09-29 – 2001-10-14, 2001-12-22 – 2002-07-...   \n1     1992-09-27 – 2002-10-13, 2002-12-22 – 2003-02-...   \n2     1992-10-09 – 2002-08-07, 2003-02-11 – 2003-02-...   \n3     1992-09-27 – 2002-08-05, 2004-12-24 – 2005-01-...   \n4                               2006-03-20 – 2025-07-20   \n...                                                 ...   \n4195   2009-12-12 – 2010-02-07, 2012-08-09 – 2021-05-04   \n4196  1986-09-30 – 2009-07-29, 2012-10-23 – 2013-01-...   \n4197  1995-12-31 – 2008-09-17, 2012-10-15 – 2019-07-...   \n4198  2008-12-13 – 2016-09-28, 2016-12-06 – 2016-12-...   \n4199  1992-11-30 – 1992-12-30, 1993-03-29 – 1993-06-...   \n\n                                        Storage_Periods  \\\n0                                                  None   \n1                                                  None   \n2                                                  None   \n3                                                  None   \n4                                                  None   \n...                                                 ...   \n4195   2009-12-12 – 2010-02-07, 2012-08-09 – 2021-05-04   \n4196  1986-09-30 – 2009-07-29, 2012-10-23 – 2013-01-...   \n4197  1995-12-31 – 2008-09-17, 2012-10-15 – 2019-07-...   \n4198                                               None   \n4199                                               None   \n\n     Level_Record_Length_Years Storage_Record_Length_Years  ...  \\\n0                         25.1                        None  ...   \n1                         28.4                        None  ...   \n2                         24.5                        None  ...   \n3                         24.5                        None  ...   \n4                         19.4                        None  ...   \n...                        ...                         ...  ...   \n4195                       8.9                         8.9  ...   \n4196                      31.2                        31.2  ...   \n4197                      20.5                        20.5  ...   \n4198                      12.7                        None  ...   \n4199                      18.8                        None  ...   \n\n     Reservoirs_fid_1 Reservoirs_Match_Distance_m GDW_Match_Type GDW_GDW_ID  \\\n0                 NaN                         NaN            NaN        NaN   \n1                 NaN                         NaN            NaN        NaN   \n2                 NaN                         NaN            NaN        NaN   \n3                 NaN                         NaN            NaN        NaN   \n4             19680.0                    0.000000         buffer    26844.0   \n...               ...                         ...            ...        ...   \n4195          78451.0                    0.000000         within     5385.0   \n4196          84659.0                    1.328184         buffer     5397.0   \n4197              NaN                         NaN            NaN        NaN   \n4198              NaN                         NaN            NaN        NaN   \n4199              NaN                         NaN            NaN        NaN   \n\n     GDW_Match_Distance_m            Type      Country   Source  \\\n0                     NaN  Remote sensing          USA  G-REALM   \n1                     NaN  Remote sensing          USA  G-REALM   \n2                     NaN  Remote sensing          USA  G-REALM   \n3                     NaN  Remote sensing          USA  G-REALM   \n4               58.879466         Station          USA     USGS   \n...                   ...             ...          ...      ...   \n4195             0.000000         Station    Australia      WDO   \n4196           291.396217         Station    Australia      WDO   \n4197                  NaN         Station    Australia      WDO   \n4198                  NaN  Remote sensing  New Zealand  G-REALM   \n4199                  NaN  Remote sensing  New Zealand  G-REALM   \n\n                                            Source_Link  \\\n0     https://ipad.fas.usda.gov/cropexplorer/global_...   \n1     https://ipad.fas.usda.gov/cropexplorer/global_...   \n2     https://ipad.fas.usda.gov/cropexplorer/global_...   \n3     https://ipad.fas.usda.gov/cropexplorer/global_...   \n4                    https://waterdata.usgs.gov/nwis/sw   \n...                                                 ...   \n4195                   http://www.bom.gov.au/waterdata/   \n4196                   http://www.bom.gov.au/waterdata/   \n4197                   http://www.bom.gov.au/waterdata/   \n4198  https://ipad.fas.usda.gov/cropexplorer/global_...   \n4199  https://ipad.fas.usda.gov/cropexplorer/global_...   \n\n                                         Other_Metadata  \n0     lat_range_min:60.992;lat_range_max:60.952;sat_...  \n1     lat_range_min:60.256;lat_range_max:60.374;sat_...  \n2     lat_range_min:60.813;lat_range_max:60.792;sat_...  \n3     lat_range_min:60.756;lat_range_max:60.795;sat_...  \n4        Kauai County, Hawaii, Hydrologic Unit 20070000  \n...                                                 ...  \n4195  QLD - Queensland Bulk Water Supply Authority (...  \n4196  QLD - Queensland Bulk Water Supply Authority (...  \n4197  QLD - Queensland Bulk Water Supply Authority (...  \n4198  lat_range_min:-44.987;lat_range_max:-44.934;sa...  \n4199  lat_range_min:-38.863;lat_range_max:-38.799;sa...  \n\n[4200 rows x 28 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RES_ID</th>\n      <th>Station</th>\n      <th>Name</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>Temporal_Resolution</th>\n      <th>Level_Periods</th>\n      <th>Storage_Periods</th>\n      <th>Level_Record_Length_Years</th>\n      <th>Storage_Record_Length_Years</th>\n      <th>...</th>\n      <th>Reservoirs_fid_1</th>\n      <th>Reservoirs_Match_Distance_m</th>\n      <th>GDW_Match_Type</th>\n      <th>GDW_GDW_ID</th>\n      <th>GDW_Match_Distance_m</th>\n      <th>Type</th>\n      <th>Country</th>\n      <th>Source</th>\n      <th>Source_Link</th>\n      <th>Other_Metadata</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>001886</td>\n      <td>Kagalurpak</td>\n      <td>60.970000</td>\n      <td>-164.010000</td>\n      <td>Daily</td>\n      <td>1992-09-29 – 2001-10-14, 2001-12-22 – 2002-07-...</td>\n      <td>None</td>\n      <td>25.1</td>\n      <td>None</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Remote sensing</td>\n      <td>USA</td>\n      <td>G-REALM</td>\n      <td>https://ipad.fas.usda.gov/cropexplorer/global_...</td>\n      <td>lat_range_min:60.992;lat_range_max:60.952;sat_...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>000497</td>\n      <td>Dall</td>\n      <td>60.330000</td>\n      <td>-163.800000</td>\n      <td>Daily</td>\n      <td>1992-09-27 – 2002-10-13, 2002-12-22 – 2003-02-...</td>\n      <td>None</td>\n      <td>28.4</td>\n      <td>None</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Remote sensing</td>\n      <td>USA</td>\n      <td>G-REALM</td>\n      <td>https://ipad.fas.usda.gov/cropexplorer/global_...</td>\n      <td>lat_range_min:60.256;lat_range_max:60.374;sat_...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>001469</td>\n      <td>Baird_Inlet</td>\n      <td>60.790000</td>\n      <td>-163.490000</td>\n      <td>Daily</td>\n      <td>1992-10-09 – 2002-08-07, 2003-02-11 – 2003-02-...</td>\n      <td>None</td>\n      <td>24.5</td>\n      <td>None</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Remote sensing</td>\n      <td>USA</td>\n      <td>G-REALM</td>\n      <td>https://ipad.fas.usda.gov/cropexplorer/global_...</td>\n      <td>lat_range_min:60.813;lat_range_max:60.792;sat_...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>001063</td>\n      <td>Nunavakpak</td>\n      <td>60.770000</td>\n      <td>-162.570000</td>\n      <td>Daily</td>\n      <td>1992-09-27 – 2002-08-05, 2004-12-24 – 2005-01-...</td>\n      <td>None</td>\n      <td>24.5</td>\n      <td>None</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Remote sensing</td>\n      <td>USA</td>\n      <td>G-REALM</td>\n      <td>https://ipad.fas.usda.gov/cropexplorer/global_...</td>\n      <td>lat_range_min:60.756;lat_range_max:60.795;sat_...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>16094150</td>\n      <td>Ka Loko Reservoir near Kilauea, Kauai, HI</td>\n      <td>22.178968</td>\n      <td>-159.379061</td>\n      <td>Daily</td>\n      <td>2006-03-20 – 2025-07-20</td>\n      <td>None</td>\n      <td>19.4</td>\n      <td>None</td>\n      <td>...</td>\n      <td>19680.0</td>\n      <td>0.000000</td>\n      <td>buffer</td>\n      <td>26844.0</td>\n      <td>58.879466</td>\n      <td>Station</td>\n      <td>USA</td>\n      <td>USGS</td>\n      <td>https://waterdata.usgs.gov/nwis/sw</td>\n      <td>Kauai County, Hawaii, Hydrologic Unit 20070000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4195</th>\n      <td>4196</td>\n      <td>142108A</td>\n      <td>Sideling Ck at Lake Kurwongbah Dam HW</td>\n      <td>-27.250000</td>\n      <td>152.950000</td>\n      <td>Daily</td>\n      <td>2009-12-12 – 2010-02-07, 2012-08-09 – 2021-05-04</td>\n      <td>2009-12-12 – 2010-02-07, 2012-08-09 – 2021-05-04</td>\n      <td>8.9</td>\n      <td>8.9</td>\n      <td>...</td>\n      <td>78451.0</td>\n      <td>0.000000</td>\n      <td>within</td>\n      <td>5385.0</td>\n      <td>0.000000</td>\n      <td>Station</td>\n      <td>Australia</td>\n      <td>WDO</td>\n      <td>http://www.bom.gov.au/waterdata/</td>\n      <td>QLD - Queensland Bulk Water Supply Authority (...</td>\n    </tr>\n    <tr>\n      <th>4196</th>\n      <td>4197</td>\n      <td>146033A</td>\n      <td>Nerang R at Hinze Dam HW</td>\n      <td>-28.050000</td>\n      <td>153.280000</td>\n      <td>Daily</td>\n      <td>1986-09-30 – 2009-07-29, 2012-10-23 – 2013-01-...</td>\n      <td>1986-09-30 – 2009-07-29, 2012-10-23 – 2013-01-...</td>\n      <td>31.2</td>\n      <td>31.2</td>\n      <td>...</td>\n      <td>84659.0</td>\n      <td>1.328184</td>\n      <td>buffer</td>\n      <td>5397.0</td>\n      <td>291.396217</td>\n      <td>Station</td>\n      <td>Australia</td>\n      <td>WDO</td>\n      <td>http://www.bom.gov.au/waterdata/</td>\n      <td>QLD - Queensland Bulk Water Supply Authority (...</td>\n    </tr>\n    <tr>\n      <th>4197</th>\n      <td>4198</td>\n      <td>146034A</td>\n      <td>Little Nerang</td>\n      <td>-28.145197</td>\n      <td>153.284782</td>\n      <td>Daily</td>\n      <td>1995-12-31 – 2008-09-17, 2012-10-15 – 2019-07-...</td>\n      <td>1995-12-31 – 2008-09-17, 2012-10-15 – 2019-07-...</td>\n      <td>20.5</td>\n      <td>20.5</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Station</td>\n      <td>Australia</td>\n      <td>WDO</td>\n      <td>http://www.bom.gov.au/waterdata/</td>\n      <td>QLD - Queensland Bulk Water Supply Authority (...</td>\n    </tr>\n    <tr>\n      <th>4198</th>\n      <td>4199</td>\n      <td>100265</td>\n      <td>Wakatipu</td>\n      <td>-44.960000</td>\n      <td>168.400000</td>\n      <td>Daily</td>\n      <td>2008-12-13 – 2016-09-28, 2016-12-06 – 2016-12-...</td>\n      <td>None</td>\n      <td>12.7</td>\n      <td>None</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Remote sensing</td>\n      <td>New Zealand</td>\n      <td>G-REALM</td>\n      <td>https://ipad.fas.usda.gov/cropexplorer/global_...</td>\n      <td>lat_range_min:-44.987;lat_range_max:-44.934;sa...</td>\n    </tr>\n    <tr>\n      <th>4199</th>\n      <td>4200</td>\n      <td>000511</td>\n      <td>Taupo</td>\n      <td>-38.800000</td>\n      <td>176.030000</td>\n      <td>Daily</td>\n      <td>1992-11-30 – 1992-12-30, 1993-03-29 – 1993-06-...</td>\n      <td>None</td>\n      <td>18.8</td>\n      <td>None</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Remote sensing</td>\n      <td>New Zealand</td>\n      <td>G-REALM</td>\n      <td>https://ipad.fas.usda.gov/cropexplorer/global_...</td>\n      <td>lat_range_min:-38.863;lat_range_max:-38.799;sa...</td>\n    </tr>\n  </tbody>\n</table>\n<p>4200 rows × 28 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------- 配置 --------\n",
    "data_dir = Path(\"../GROWL/GROWL_timeseries\")\n",
    "\n",
    "# 最终导出的列（学术化命名）\n",
    "FINAL_COLS = [\n",
    "    \"RES_ID\", \"Station\", \"Name\", \"Latitude\", \"Longitude\",\n",
    "    \"Temporal_Resolution\",\n",
    "    \"Level_Periods\", \"Storage_Periods\",\n",
    "    \"Level_Record_Length_Years\", \"Storage_Record_Length_Years\",\n",
    "    \"In-situ_Level_Ratio\", \"In-situ_Storage_Ratio\",\n",
    "    \"Station_Elevation_m\", \"Vertical_Datum\",\n",
    "    \"Hydrolakes_Match_Type\", \"Hydrolakes_Hylak_id\", \"Hydrolakes_Match_Distance_m\",\n",
    "    \"Reservoirs_Match_Type\", \"Reservoirs_fid_1\", \"Reservoirs_Match_Distance_m\",\n",
    "    \"GDW_Match_Type\", \"GDW_GDW_ID\", \"GDW_Match_Distance_m\",\n",
    "    \"Type\", \"Country\", \"Source\", \"Source_Link\", \"Other_Metadata\"]\n",
    "\n",
    "# 若原始 metadata 的部分列名需要映射到最终名，这里一次性给出\n",
    "ORIG_TO_FINAL = {\n",
    "    \"Altitude\": \"Station_Elevation_m\",\n",
    "    \"AltitudeReference\": \"Vertical_Datum\",\n",
    "    \"Hydrolakes_match_type\": \"Hydrolakes_Match_Type\",\n",
    "    \"Hydrolakes_Hylak_id\": \"Hydrolakes_Hylak_id\",\n",
    "    \"Hydrolakes_match_distance_m\": \"Hydrolakes_Match_Distance_m\",\n",
    "    \"reservoirs_match_type\": \"Reservoirs_Match_Type\",\n",
    "    \"reservoirs_fid_1\": \"Reservoirs_fid_1\",\n",
    "    \"reservoirs_match_distance_m\": \"Reservoirs_Match_Distance_m\",\n",
    "    \"GDW_match_type\": \"GDW_Match_Type\",\n",
    "    \"GDW_GDW_ID\": \"GDW_GDW_ID\",\n",
    "    \"GDW_match_distance_m\": \"GDW_Match_Distance_m\",\n",
    "    \"Link\": \"Source_Link\",\n",
    "    \"Other\": \"Other_Metadata\"}\n",
    "\n",
    "# -------- 工具函数 --------\n",
    "def get_continuous_periods(dates, freq=\"D\"):\n",
    "    \"\"\"输入日期序列，返回连续区间字符串\"\"\"\n",
    "    dates = pd.Series(dates).dropna().sort_values().dt.date\n",
    "    if dates.empty:\n",
    "        return None\n",
    "\n",
    "    periods = []\n",
    "    start = dates.iloc[0]\n",
    "    prev = start\n",
    "\n",
    "    for current in dates.iloc[1:]:\n",
    "        if freq == \"D\":\n",
    "            continuous = ((current - prev).days == 1)\n",
    "        elif freq == \"M\":\n",
    "            # 月频的“相邻一月”判断\n",
    "            continuous = (\n",
    "                (current.year == prev.year and current.month == prev.month + 1) or\n",
    "                (prev.month == 12 and current.year == prev.year + 1 and current.month == 1)\n",
    "            )\n",
    "        else:\n",
    "            continuous = False\n",
    "\n",
    "        if not continuous:\n",
    "            periods.append(f\"{start:%Y-%m-%d} – {prev:%Y-%m-%d}\")\n",
    "            start = current\n",
    "        prev = current\n",
    "\n",
    "    periods.append(f\"{start:%Y-%m-%d} – {prev:%Y-%m-%d}\")\n",
    "    return \", \".join(periods)\n",
    "\n",
    "def count_to_years(count, freq):\n",
    "    \"\"\"根据数据点数和频率换算成年数（保留 1 位小数）\"\"\"\n",
    "    if not count or count == 0:\n",
    "        return None\n",
    "    if freq == \"D\":\n",
    "        return round(count / 365, 1)\n",
    "    if freq == \"M\":\n",
    "        return round(count / 12, 1)\n",
    "    return None\n",
    "\n",
    "def infer_temporal_resolution(dates) -> str:\n",
    "    \"\"\"根据单月内样本量粗判日/月，并返回 'Daily' 或 'Monthly'\"\"\"\n",
    "    s = pd.Series(dates)\n",
    "    counts = s.groupby([s.dt.year, s.dt.month]).count()\n",
    "    if (counts >= 10).any():\n",
    "        return \"Daily\"\n",
    "    return \"Monthly\"\n",
    "\n",
    "# 读取原始 metadata 并把能直接映射的列换名到最终名（其余原样保留）\n",
    "df_meta_raw = pd.read_csv(r\"../Metadata.csv\")\n",
    "df_meta = df_meta_raw.rename(columns=ORIG_TO_FINAL).copy()\n",
    "\n",
    "# 先确保最终列都存在，默认 None（不会覆盖已有同名列）\n",
    "for col in FINAL_COLS:\n",
    "    if col not in df_meta.columns:\n",
    "        df_meta[col] = None\n",
    "\n",
    "# 遍历逐站填充“计算类”字段\n",
    "for idx, row in df_meta.iterrows():\n",
    "    ID = str(row[\"RES_ID\"])\n",
    "    file_path = data_dir / f\"{ID}.csv\"\n",
    "    if not file_path.exists():\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path, dtype={0: str})\n",
    "    if \"Date\" not in df.columns:\n",
    "        continue\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "    # 判定时间分辨率\n",
    "    temporal_res = infer_temporal_resolution(df[\"Date\"])\n",
    "    df_meta.at[idx, \"Temporal_Resolution\"] = temporal_res\n",
    "\n",
    "    # Level\n",
    "    if {\"Level\", \"Flag_Level\"}.issubset(df.columns):\n",
    "        total = df[\"Flag_Level\"].notna().sum()\n",
    "        zero_ratio = (df[\"Flag_Level\"] == 0).sum() / total if total > 0 else 0\n",
    "        df_meta.at[idx, \"In-situ_Level_Ratio\"] = zero_ratio\n",
    "\n",
    "        non_na = df[\"Level\"].notna()\n",
    "        if non_na.any():\n",
    "            # 连续区间（按判定的时间分辨率计算相邻）\n",
    "            freq_code = \"D\" if temporal_res == \"Daily\" else \"M\"\n",
    "            df_meta.at[idx, \"Level_Periods\"] = get_continuous_periods(df.loc[non_na, \"Date\"], freq=freq_code)\n",
    "            # 年数\n",
    "            df_meta.at[idx, \"Level_Record_Length_Years\"] = count_to_years(non_na.sum(), freq_code)\n",
    "\n",
    "    # Storage\n",
    "    if {\"Storage\", \"Flag_Storage\"}.issubset(df.columns):\n",
    "        total = df[\"Flag_Storage\"].notna().sum()\n",
    "        zero_ratio = (df[\"Flag_Storage\"] == 0).sum() / total if total > 0 else 0\n",
    "        df_meta.at[idx, \"In-situ_Storage_Ratio\"] = zero_ratio\n",
    "\n",
    "        non_na = df[\"Storage\"].notna()\n",
    "        if non_na.any():\n",
    "            freq_code = \"D\" if temporal_res == \"Daily\" else \"M\"\n",
    "            df_meta.at[idx, \"Storage_Periods\"] = get_continuous_periods(df.loc[non_na, \"Date\"], freq=freq_code)\n",
    "            df_meta.at[idx, \"Storage_Record_Length_Years\"] = count_to_years(non_na.sum(), freq_code)\n",
    "\n",
    "# 只保留最终列并导出\n",
    "df_Metadata = df_meta[FINAL_COLS].copy()\n",
    "df_Metadata.to_csv(r\"../GROWL/GROWL_metadata.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_Metadata"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-18T11:27:33.708122400Z",
     "start_time": "2025-10-18T11:25:11.598087800Z"
    }
   },
   "id": "bfbe72f658c0b6a2",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eab646980ab26151"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
